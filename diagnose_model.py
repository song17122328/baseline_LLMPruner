#!/usr/bin/env python3
"""
è¯Šæ–­å‰ªæžåŽæ¨¡åž‹çš„ç»´åº¦é…ç½®
æ£€æŸ¥attentionå±‚çš„ç»´åº¦æ˜¯å¦æ­£ç¡®åŒ¹é…
"""

import torch
import sys

def diagnose_pruned_model(ckpt_path):
    """æ£€æŸ¥å‰ªæžåŽæ¨¡åž‹çš„ç»´åº¦é…ç½®"""
    print(f"åŠ è½½æ¨¡åž‹: {ckpt_path}")
    pruned_dict = torch.load(ckpt_path, map_location='cpu', weights_only=False)
    model = pruned_dict['model']

    print(f"\n{'='*80}")
    print("æ¨¡åž‹é…ç½®:")
    print(f"{'='*80}")
    config = model.config
    print(f"åŽŸå§‹é…ç½®:")
    print(f"  hidden_size: {config.hidden_size}")
    print(f"  num_attention_heads: {config.num_attention_heads}")
    print(f"  num_key_value_heads: {config.num_key_value_heads}")
    print(f"  intermediate_size: {config.intermediate_size}")
    print(f"  head_dim: {config.hidden_size // config.num_attention_heads}")

    print(f"\n{'='*80}")
    print("å„å±‚å®žé™…ç»´åº¦æ£€æŸ¥ (Layers 0-31):")
    print(f"{'='*80}")

    issues = []

    for layer_idx, layer in enumerate(model.model.layers):
        attn = layer.self_attn

        # èŽ·å–å®žé™…æƒé‡ç»´åº¦
        q_dim = attn.q_proj.weight.data.shape[0]
        k_dim = attn.k_proj.weight.data.shape[0]
        v_dim = attn.v_proj.weight.data.shape[0]
        o_in_dim = attn.o_proj.weight.data.shape[1]

        # èŽ·å–é…ç½®çš„ç»´åº¦
        configured_num_heads = attn.num_heads
        configured_num_kv_heads = attn.num_key_value_heads
        configured_head_dim = attn.head_dim

        # è®¡ç®—æœŸæœ›ç»´åº¦
        expected_q_dim = configured_num_heads * configured_head_dim
        expected_kv_dim = configured_num_kv_heads * configured_head_dim

        # æ£€æŸ¥æ˜¯å¦åŒ¹é…
        q_match = (q_dim == expected_q_dim)
        k_match = (k_dim == expected_kv_dim)
        v_match = (v_dim == expected_kv_dim)
        o_match = (o_in_dim == q_dim)

        all_match = q_match and k_match and v_match and o_match

        if not all_match or layer_idx in [0, 1, 2, 3, 4, 15, 29, 30, 31]:
            # æ‰“å°æœ‰é—®é¢˜çš„å±‚ï¼Œæˆ–è€…è¾¹ç•Œå±‚
            status = "âœ“" if all_match else "âœ—"
            print(f"\nLayer {layer_idx} {status}:")
            print(f"  é…ç½®: num_heads={configured_num_heads}, num_kv_heads={configured_num_kv_heads}, head_dim={configured_head_dim}")
            print(f"  æœŸæœ›ç»´åº¦: Q={expected_q_dim}, K/V={expected_kv_dim}")
            print(f"  å®žé™…ç»´åº¦: Q={q_dim} {'âœ“' if q_match else 'âœ—'}, K={k_dim} {'âœ“' if k_match else 'âœ—'}, V={v_dim} {'âœ“' if v_match else 'âœ—'}, O_in={o_in_dim} {'âœ“' if o_match else 'âœ—'}")

            if not all_match:
                issues.append({
                    'layer': layer_idx,
                    'q_dim': q_dim,
                    'k_dim': k_dim,
                    'v_dim': v_dim,
                    'o_in_dim': o_in_dim,
                    'expected_q': expected_q_dim,
                    'expected_kv': expected_kv_dim,
                    'configured_heads': configured_num_heads,
                    'configured_kv_heads': configured_num_kv_heads,
                    'head_dim': configured_head_dim
                })

    # MLPç»´åº¦æ£€æŸ¥
    print(f"\n{'='*80}")
    print("MLPç»´åº¦æ£€æŸ¥ (æŠ½æ ·):")
    print(f"{'='*80}")

    for layer_idx in [0, 4, 15, 29, 31]:
        layer = model.model.layers[layer_idx]
        mlp = layer.mlp

        gate_dim = mlp.gate_proj.weight.data.shape[0]
        up_dim = mlp.up_proj.weight.data.shape[0]
        down_in_dim = mlp.down_proj.weight.data.shape[1]

        mlp_match = (gate_dim == up_dim == down_in_dim)
        status = "âœ“" if mlp_match else "âœ—"

        print(f"Layer {layer_idx} {status}: gate={gate_dim}, up={up_dim}, down_in={down_in_dim}")

    # æ€»ç»“
    print(f"\n{'='*80}")
    print("è¯Šæ–­æ€»ç»“:")
    print(f"{'='*80}")

    if issues:
        print(f"âŒ å‘çŽ° {len(issues)} ä¸ªå±‚å­˜åœ¨ç»´åº¦ä¸åŒ¹é…é—®é¢˜:")
        for issue in issues:
            print(f"\n  Layer {issue['layer']}:")
            print(f"    - é…ç½®: {issue['configured_heads']} heads, {issue['configured_kv_heads']} kv_heads, head_dim={issue['head_dim']}")
            print(f"    - Qç»´åº¦: å®žé™…={issue['q_dim']}, æœŸæœ›={issue['expected_q']}, å·®å€¼={issue['q_dim'] - issue['expected_q']}")
            print(f"    - Kç»´åº¦: å®žé™…={issue['k_dim']}, æœŸæœ›={issue['expected_kv']}, å·®å€¼={issue['k_dim'] - issue['expected_kv']}")
            print(f"    - Vç»´åº¦: å®žé™…={issue['v_dim']}, æœŸæœ›={issue['expected_kv']}, å·®å€¼={issue['v_dim'] - issue['expected_kv']}")

            # åˆ†æžåŽŸå› 
            if issue['q_dim'] % issue['head_dim'] != 0:
                print(f"    âš ï¸  Qç»´åº¦({issue['q_dim']})ä¸æ˜¯head_dim({issue['head_dim']})çš„æ•´æ•°å€ï¼")
            if issue['k_dim'] % issue['head_dim'] != 0:
                print(f"    âš ï¸  Kç»´åº¦({issue['k_dim']})ä¸æ˜¯head_dim({issue['head_dim']})çš„æ•´æ•°å€ï¼")

        print(f"\nðŸ” å¯èƒ½çš„åŽŸå› :")
        print(f"  1. å‰ªæžåŽçš„ç»´åº¦ä¸æ»¡è¶³head_dimçš„æ•´æ•°å€çº¦æŸ")
        print(f"  2. GQAçº¦æŸæœªæ­£ç¡®å¤„ç† (num_kv_headså¿…é¡»èƒ½è¢«num_headsæ•´é™¤)")
        print(f"  3. llama3.py:164-165çš„é‡æ–°è®¡ç®—é€»è¾‘æœ‰é—®é¢˜")
    else:
        print("âœ… æ‰€æœ‰å±‚çš„ç»´åº¦é…ç½®éƒ½æ­£ç¡®åŒ¹é…")
        print("\nå¦‚æžœPPLä»ç„¶å¾ˆé«˜ï¼Œå¯èƒ½çš„å…¶ä»–åŽŸå› :")
        print("  1. å‰ªæžç­–ç•¥é€‰æ‹©çš„é€šé“ä¸åˆé€‚")
        print("  2. æƒé‡åˆå¹¶ç­–ç•¥æœ‰é—®é¢˜")
        print("  3. éœ€è¦è¿›è¡ŒåŽç»­å¾®è°ƒæ‰èƒ½æ¢å¤æ€§èƒ½")

    return issues

if __name__ == '__main__':
    ckpt_path = 'prune_log/llama3_prune/pytorch_model.bin'
    if len(sys.argv) > 1:
        ckpt_path = sys.argv[1]

    issues = diagnose_pruned_model(ckpt_path)
