# Qwen å‰ªæé”™è¯¯åˆ†æä¸ä¿®å¤

## æ‚¨çš„æ—¥å¿—åˆ†æ

### ğŸ”´ ä¸»è¦é—®é¢˜ 1ï¼šå‰ªæå‰ PPL å°±å·²ç»çˆ†ç‚¸

```
PPL before pruning: {'wikitext2': 39574.39, 'ptb': 56687.69}
```

**æ­£å¸¸å€¼åº”è¯¥æ˜¯ï¼š**
- wikitext2: **8-15**
- ptb: **15-30**

**åŸå› ï¼š**
```
You are using a model of type qwen2 to instantiate a model of type llama.
This is not supported for all configurations of models and can yield errors.
```

ä»£ç ä½¿ç”¨äº† `LlamaForCausalLM.from_pretrained()` æ¥åŠ è½½ Qwen2 æ¨¡å‹ï¼Œè¿™ä¼šå¯¼è‡´ï¼š
- âœ— æ¨¡å‹ç»“æ„ä¸åŒ¹é…
- âœ— Attention æœºåˆ¶é”™ä½
- âœ— å‚æ•°æ˜ å°„é”™è¯¯
- âœ— PPL å®Œå…¨ä¸æ­£å¸¸

### ğŸ”´ ä¸»è¦é—®é¢˜ 2ï¼šå‰ªæåç»´åº¦é”™è¯¯

```
RuntimeError: shape '[4, 2048, -1, 128]' is invalid for input of size 23453696
```

å‘ç”Ÿåœ¨ï¼š`query_states = self.q_proj(hidden_states).view(hidden_shape).transpose(1, 2)`

**è®¡ç®—åˆ†æï¼š**
```python
input_size = 23453696
batch_size = 4
seq_len = 2048
head_dim = 128

# å®é™…çš„ num_heads
actual_heads = 23453696 / (4 * 2048 * 128) = 22.39...  # â† ä¸æ˜¯æ•´æ•°ï¼

# ä½†ä»£ç æœŸæœ›çš„ num_heads å¯èƒ½ä»ç„¶æ˜¯åŸå§‹å€¼ 28
# è¿™å¯¼è‡´ view() æ— æ³•æ­£ç¡® reshape
```

**åŸå› ï¼š**
- q_proj çš„è¾“å‡ºç»´åº¦è¢«å‰ªæäº†ï¼ˆä» 3584 å‡å°‘åˆ°æŸä¸ªå€¼ï¼‰
- ä½† `layer.self_attn.num_heads` æ²¡æœ‰æ›´æ–°
- æˆ–è€… Qwen2 ä½¿ç”¨äº†ä¸åŒçš„å±æ€§åç§°

## ä¿®å¤æ–¹æ¡ˆ

### ä¿®å¤ 1ï¼šä½¿ç”¨ AutoModelForCausalLM

**åŸä»£ç ï¼ˆllama3.py:40ï¼‰ï¼š**
```python
model = LlamaForCausalLM.from_pretrained(  # âœ— å¼ºåˆ¶ä½¿ç”¨ Llama ç±»
    args.base_model,
    torch_dtype=torch.float16,
)
```

**ä¿®å¤åï¼š**
```python
model = AutoModelForCausalLM.from_pretrained(  # âœ“ è‡ªåŠ¨æ£€æµ‹æ¨¡å‹ç±»å‹
    args.base_model,
    torch_dtype=torch.float16,
    trust_remote_code=True,  # Qwen éœ€è¦
)
```

**æ•ˆæœï¼š**
- âœ“ è‡ªåŠ¨åŠ è½½ Qwen2ForCausalLM
- âœ“ æ¨¡å‹ç»“æ„æ­£ç¡®
- âœ“ PPL æ¢å¤æ­£å¸¸
- âœ“ å…¼å®¹ Llamaã€Mistralã€Qwen ç­‰æ‰€æœ‰æ¨¡å‹

### ä¿®å¤ 2ï¼šæ”¹è¿› num_heads æ›´æ–°é€»è¾‘

**åŸä»£ç ï¼ˆllama3.py:221-222ï¼‰ï¼š**
```python
layer.self_attn.num_heads = ...
layer.self_attn.num_key_value_heads = ...
```

**é—®é¢˜ï¼š**
- Qwen2 å¯èƒ½ä½¿ç”¨ `num_attention_heads` è€Œä¸æ˜¯ `num_heads`
- model.config æ²¡æœ‰æ›´æ–°

**ä¿®å¤åï¼ˆllama3.py:219-248ï¼‰ï¼š**
```python
for layer_idx, layer in enumerate(model.model.layers):
    pruned_q_dim = layer.self_attn.q_proj.weight.data.shape[0]
    pruned_k_dim = layer.self_attn.k_proj.weight.data.shape[0]

    new_num_heads = pruned_q_dim // layer.self_attn.head_dim
    new_num_kv_heads = pruned_k_dim // layer.self_attn.head_dim

    # æ›´æ–°æ‰€æœ‰å¯èƒ½çš„å±æ€§åç§°
    if hasattr(layer.self_attn, 'num_heads'):
        layer.self_attn.num_heads = new_num_heads
    if hasattr(layer.self_attn, 'num_attention_heads'):
        layer.self_attn.num_attention_heads = new_num_heads
    if hasattr(layer.self_attn, 'num_key_value_heads'):
        layer.self_attn.num_key_value_heads = new_num_kv_heads

# æ›´æ–° model.config
if hasattr(model.config, 'num_attention_heads'):
    model.config.num_attention_heads = first_layer_num_heads
if hasattr(model.config, 'num_key_value_heads'):
    model.config.num_key_value_heads = first_layer_num_kv_heads
```

**æ•ˆæœï¼š**
- âœ“ æ”¯æŒä¸åŒæ¨¡å‹çš„å±æ€§åç§°
- âœ“ åŒæ—¶æ›´æ–° layer å’Œ config
- âœ“ view() reshape ä¸ä¼šå‡ºé”™

## ä¿®å¤åçš„é¢„æœŸæ—¥å¿—

ç°åœ¨è¿è¡Œç›¸åŒçš„å‘½ä»¤ï¼Œæ‚¨åº”è¯¥çœ‹åˆ°ï¼š

```bash
CUDA_VISIBLE_DEVICES=7 python llama3.py --pruning_ratio 0.20 \
  --device cuda --eval_device cuda \
  --base_model /newdata/LLMs/Qwen2.5-7B \
  --block_wise \
  --block_mlp_layer_start 4 --block_mlp_layer_end 28 \
  --block_attention_layer_start 4 --block_attention_layer_end 28 \
  --save_ckpt_log_name Qwen_conservative_20 \
  --pruner_type taylor --taylor param_first \
  --max_seq_len 2048 \
  --save_model
```

**é¢„æœŸè¾“å‡ºï¼š**
```
Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4/4 [00:14<00:00]
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 37/37 [00:31<00:00]
{'wikitext2': 10.5}  â† âœ“ æ­£å¸¸èŒƒå›´ï¼
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 11/11 [00:08<00:00]
{'wikitext2': 10.5, 'ptb': 18.2}  â† âœ“ æ­£å¸¸èŒƒå›´ï¼

PPL before pruning: {'wikitext2': 10.5, 'ptb': 18.2}  â† âœ“ æ­£å¸¸ï¼

Detected 28 layers in the model
Detected GQA configuration:
  - num_attention_heads: 28
  - num_key_value_heads: 4
  - GQA ratio: 7.0:1
âš ï¸  High GQA ratio detected (7.0:1)
âš ï¸  Skipping consecutive_groups for attention layers
  - Reason: pruned dims (102) < head_dim (128)

Start Pruning
Loss = 10.48
After Iter 1/1, #parameters: 6495989248
Updated num_heads after pruning. Example layer 0: num_heads=22
Updated model.config.num_attention_heads: 22
Updated model.config.num_key_value_heads: 3

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 37/37 [00:31<00:00]
{'wikitext2': 12.8}  â† âœ“ åˆç†çš„å¢é•¿
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 11/11 [00:08<00:00]
{'wikitext2': 12.8, 'ptb': 21.5}  â† âœ“ åˆç†çš„å¢é•¿

PPL after pruning: {'wikitext2': 12.8, 'ptb': 21.5}  â† âœ“ å¯æ¥å—ï¼
```

## å…³é”®æ”¹è¿›

### 1. PPL æ¢å¤æ­£å¸¸
- **ä¹‹å‰**ï¼šwikitext2 = 39574ï¼ˆå®Œå…¨é”™è¯¯ï¼‰
- **ä¹‹å**ï¼šwikitext2 â‰ˆ 10-12ï¼ˆæ­£å¸¸ï¼‰

### 2. å‰ªæå PPL å¢é•¿åˆç†
- **é¢„æœŸå¢é•¿**ï¼š20-30%ï¼ˆä» 10.5 â†’ 12.8ï¼‰
- **ä¸å¯æ¥å—**ï¼š>100%ï¼ˆä» 10.5 â†’ 1000+ï¼‰

### 3. æ²¡æœ‰è¿è¡Œæ—¶é”™è¯¯
- âœ“ ä¸å†å‡ºç° shape invalid é”™è¯¯
- âœ“ num_heads æ­£ç¡®æ›´æ–°
- âœ“ æ¨¡å‹å¯ä»¥æ­£å¸¸æ¨ç†

## å»ºè®®çš„æµ‹è¯•æ­¥éª¤

### 1. é¦–å…ˆæµ‹è¯• MLP-onlyï¼ˆæœ€å®‰å…¨ï¼‰

```bash
CUDA_VISIBLE_DEVICES=7 python llama3.py --pruning_ratio 0.25 \
  --device cuda --eval_device cuda \
  --base_model /newdata/LLMs/Qwen2.5-7B \
  --block_wise \
  --block_mlp_layer_start 4 --block_mlp_layer_end 28 \
  --block_attention_layer_start 28 --block_attention_layer_end 28 \
  --save_ckpt_log_name Qwen_mlp_only_25 \
  --pruner_type taylor --taylor param_first \
  --max_seq_len 2048 \
  --save_model
```

**é¢„æœŸç»“æœï¼š**
- PPL å¢é•¿ < 15%
- å‚æ•°å‡å°‘ 15-20%
- ä¸ä¼šç ´å attention

### 2. å¦‚æœ MLP-only æˆåŠŸï¼Œå†å°è¯•è”åˆå‰ªæ

ä½¿ç”¨æ‚¨åŸæ¥çš„å‘½ä»¤ï¼ˆpruning_ratio=0.20ï¼Œå‰ªæ attention 4-28ï¼‰

### 3. éªŒè¯å‰ªæç»“æœ

```bash
# æ£€æŸ¥ GQA ç»“æ„
python diagnose_qwen_gqa.py prune_log/Qwen_conservative_20/pytorch_model.bin pruned

# æµ‹è¯•ç”Ÿæˆ
python generate.py --model_type pruneLLM \
  --ckpt prune_log/Qwen_conservative_20/pytorch_model.bin
```

## æ€»ç»“

æ‚¨é‡åˆ°çš„ä¸¤ä¸ªä¸»è¦é—®é¢˜ï¼š

1. **æ¨¡å‹åŠ è½½é”™è¯¯** â†’ ä½¿ç”¨ `AutoModelForCausalLM` ä¿®å¤
2. **num_heads ä¸åŒ¹é…** â†’ æ”¹è¿›æ›´æ–°é€»è¾‘ä¿®å¤

ç°åœ¨ä»£ç åº”è¯¥èƒ½å¤Ÿï¼š
- âœ“ æ­£ç¡®åŠ è½½ Qwen2 æ¨¡å‹
- âœ“ æ­£ç¡®å‰ªæ
- âœ“ æ­£ç¡®æ›´æ–° num_heads
- âœ“ äº§ç”Ÿåˆç†çš„ PPL

è¯·é‡æ–°è¿è¡Œå‘½ä»¤æµ‹è¯•ï¼ğŸš€
